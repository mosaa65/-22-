
--- Lec 1 Lect 2 Lec 3.pdf page 44 ---
Database Management System

تعريف نظام إدارة قاعدة البيانات

ما هى إدارة نظام قاعدة البيانات ‎Database Management Information System) DBMS‏ ) ؟
إمكانية ‎ald‏ عدد كبير من المستخدمين من الوصول والتعامل مع البيانات» وينظر إليها كذلك على أنها
حلقة الوصل ببن المستخدمين وقاعدة البيانات» بحيث تقوم باستقيال متطلبات المستخدمين ومن ثم نقلها

إلى قاعدة البيانات وتنفيذ البرامج اللازمة لتنفين هذه المتطلبات ومن ثم تزويد المستخدم بالنتائج المطلوية.

—-
—
DATABASE

= ©

Wm Techopedia



--- Lec 1 Lect 2 Lec 3.pdf page 45 ---
RDBMS Life Cycle

Requirement Database Evaluation
Analysis Design and selection
Growth and Logical
Change Database Design
RDBMS LIFECYCLE
Operate and Physical
Maintain Database Design
Testing and
Performance Data Loading Implementation
Tuning

Figure 5.2 RBDMS life cycle.



--- Lec 1 Lect 2 Lec 3.pdf page 46 ---
Components of Database Management System

تشتمل ‎dan‏ نظام قاعدة البيانات على مكونات مختلفة تدير قاعدة البيانات وتتفاعل معها. تضمن هذه

المكونات تخزين البيانات واسترجاعها وإدارتها بكفاءة.

1. قاعدة البيانات ‎Database‏ المكون المركزي للبيئة» قاعدة البيانات» حيث يتم تنظيم البيانات
وتنظيمها. ويتضمن الجداول والصفوف والأعمدة والعلاقات التي تتبع مخططًا محددًا أو نموذج

2. نظام إدارة قواعد البيانات (081/15): نظام إدارة قواعد البيانات هو البرنامج المسؤول عن إدارة
قاعدة البيانات والتحكم فيها. يعمل كوسيط بين المستخدمين أو التطبيقات وقاعدة البيانات نفسها. يقوم
نظام إدارة قواعد البيانات ‎(DBMS)‏ بمهام تخزين البيانات واسترجاعها وأمنها والتحكم في التزامن
وادارة المعامللات. تتضمن أمثلة برامج إدارة قواعد البيانات ‎Oracle.» PostgreSQL.» MySQL‏ و ‎SQL‏

a ٠. “
‏ل‎ ete


--- Lect  4 Big Data2 _011245.pdf page 5 ---
Big Data Life Cycle


--- Lect  4 Big Data2 _011245.pdf page 8 ---
Consumption

Information Exploration Layer

Data Visualization

Real-Time Monitoring

مو سس

ا

Decision Support

Ce >

ee

Transformation

Analytics Layer

MapReduce
Task
i — e 5 ag .
MAP ‏اله‎ MAP jj. | MAP 1
~ YT a
Processing

5 قم
‎Reduce Reduce‏

Stream Computing

DataBase Analytics

Capturing

Data Layer Data Aggregation Layer

meni Data Acquisition

* ih
Data Sources Data Format ml 4

Online Banking

OG & structured Data = Data preprocessing
See 2 ‏تست ه‎
Social Media

Cleaning Integration
oS =
a ‏كم‎ cmd

SS BS ==] Unstructured Data

Patient Records
Pa
—
| —

| Transfonmation
Data Storage ne >

BMS il so >

ولك

Point of sales Semi-Structured Data

Master Data Management Data Lifecycle Management
Data Data Data Data Archive Data Warehouse 1 |
Immediacy Completeness Accuracy Availability Data Maintanence Data Deletion
Data Security and Privacy Management
Sensitive Data Security Activity Access Protecting Data [ Auditing and
Discovery Policies Monitoring Management in Transit

Compliance Reporting

Figure 1.10 Big data Life cycle.



--- Lect  4 Big Data2 _011245.pdf page 13 ---
Big Data Source ‏مصادر البيانات الضخمة‎

Twitter 2 &S Point of sale
Facebook 00 \ ly Frise} YouTube

Weblog ‏ليد له‎ <-> E-mail

BIG
DATA

© eBay )e Sensors

Figure 1.5 Sources of big data.

Documents

Patient
Monitor



--- Lect  4 Big Data2 _011245.pdf page 27 ---
Data Preprocessing


--- Lect  4 Big Data2 _011245.pdf page 28 ---
Data Preprocessing

Data
Integration

Data
Transforma-
tion



--- Lect  4 Big Data2 _011245.pdf page 29 ---
Data Preprocessing


--- Lect  4 Big Data2 _011245.pdf page 61 ---
Quartiles, The Interquartile Range, And Outliers

quartiles
- divide a data set into 4 equal groups
- are marked Q:, 02, and Qs;

lowest median highest
data value 5 Q> O: data value

25% 25% 25% 25%
DI 25th percentile
— sS«SOth percentile
DES 75th percentile

Finding data values that correspond to Qi, Q2, Q3


--- Lect  4 Big Data2 _011245.pdf page 62 ---
Quartiles, The Interquartile Range, And Outliers

quartiles
- divide a data set into 4 equal groups
- are marked Q:, 02, and 0

lowest median highest
data value Q1 Q> Os data value

25% 25% 25% 25%
DP 25th percentile
«50th percentile
as 75th percentile

Finding data values that correspond to 01, 02, 3


--- Lect  4 Big Data2 _011245.pdf page 63 ---
Quartiles, The Interquartile Range, And Outliers

0 lowest median highest
Finding data values that correspond to Qi, Q2, 3 datavalue ‏م‎ Qe ‏ون‎ data value

25% 25% 25% 25%
value 10 value 11

arrange the data smallest to largest
56789 10 11 12 145 1601117 18 19 21 23 24 25 27 31

median

dian = -
median = Qa 20 values in

the data set



--- Lect  4 Big Data2 _011245.pdf page 64 ---
highest
data value

25%

Quartiles, The Interquartile Range, And Outliers

١ . lowest median
Finding data values that correspond to Qi, 02, 3 datavalue ‏يم‎ 47

25% 25% 25%
value 10 value 11
arrange the data smallest to largest

56789 10 11 12 14 15 16017 15 19 21 23 24 25 27 31

di
mealan 15+16 -155

2

median =Q2 =15;5 20 values in

the data set


--- Lect  4 Big Data2 _011245.pdf page 65 ---
highest
data value

25%

Quartiles, The Interquartile Range, And Outliers

١ ١ lowest median
Finding data values that correspond to Qi, 02, 3 datavalue ‏يم‎ 47

25% 25% 25%

value 10 value 11
value 6
value 5

5 6 7 8,9 10,11 12 14 15 16 17 18 19 21 23 24 25 27 31

لطا

median

10 total values
median = Q2 = 15.5

median of values that fall below Q2 = Qi =


--- Lect  4 Big Data2 _011245.pdf page 66 ---
highest
data value

25%

Quartiles, The Interquartile Range, And Outliers

lowest median

Finding data values that correspond to Qi, 02, 3 datavalue = q, Q2
25% 25% 25%

value 10 value 11
value 6
value 5

567 3503 10) 11 12 14 15 16 17 18 19 21 23 24 25 27 31

median 9 — - 5

10 total values
median = Q2 = 15.5

median of values that fall below Q2 =Q =


--- Lect  4 Big Data2 _011245.pdf page 67 ---
highest
data value

25%

Quartiles, The Interquartile Range, And Outliers

١ ١ lowest median
Finding data values that correspond to Qi, Q2, 3 datavalue  Q, 0
25% 25% 25%
value 10 value 11
value 6 value 15
alue value 16
value 5

56789 10 11 12 14 15 16 17 18 19)21 2324 25 27 31

median
21+ 23

= 22
2

10 total values
median = Q2 = 15.5

median of values that fall below Q2 - 01 - 5
median of values that fall above Q>2 = Q3 = 22



--- Lect  4 Big Data2 _011245.pdf page 68 ---
Quartiles, The Interquartile Range, And Outliers

2 gg lowest median highest
Finding data values that correspond to Qi, 02, 3 datavalue ‏0م‎ Qe ‏ون‎ data value

25% 25% 25% 25%

56789 10 11 12 14 15 16 17 18 19 21 23 24 25 27 31

Qi=9.5
Q2=15.5
Q3 = 22

Interquartile Range (IQR)
- difference between 3rd and 1st quartiles

IQR = Q3-Q1
IQR = 22-9.5=12.5



--- Lect  4 Big Data2 _011245.pdf page 69 ---
Quartiles, The Interquartile Range, And Outliers

0 0 lowest median highest
Finding data values that correspond to Qi, 02, 3 datavalue ‏يم‎ Q> ‏ين‎ data value
Interquartile Range (IQR) 25% 25% 25% 25%

IQR = 03- 01
IQR = 22 - 9,5 - 5

01 - 5

Q>=15.5 56789 10 11 12 14 15 16 17 18 19 21 23 24 25 27 31

Q3 = 2

IQR to identify outliers
Outlier - an extremely high or low value compared to other values in the data set
***can effect the mean and standard deviation



--- Lect  4 Big Data2 _011245.pdf page 70 ---
Quartiles, The Interquartile Range, And Outliers

oa lowest median highest
Finding data values that correspond to Qi, 02, 3 datavalue ‏يم‎ Q> ‏ون‎ data value
Interquartile Range (IQR) 25% 25% 25% 25%

IQR = 03- 0

IQR = 22-9.5=12.5

Qi=9.5
Q> - 5 5 6 / 8 9 10 11 12 14 15 16 [17] 18 19 21 23 24 25 27 31
Q3 - 2 5 6 7 8 9:10 11 12 14 15,16 18 19 21 23,24 25 27 0

IQR to identify outliers

Qi=9.5 Q3= 23.5
IQR = 23.5-9.5=14
multilpy IQR by 1.5 - 14 < 1.5 - 21


--- Lect  4 Big Data2 _011245.pdf page 71 ---
Quartiles, The Interquartile Range, And Outliers

ae lowest median highest
Finding data values that correspond to Qi, 02, 3 datavalue ‏0م‎ Q2 ‏و0‎ datavalue
Interquartile Range (IQR) 25% 25% 25% 25%

IQR = 03- 01

IQR = 22-9.5=12.5

Qi=9.5
Q>=15.5 5678910 11 12 14 15 16 [17] 18 19 21 23 24 25 27 31
Q3 =22 5 6 7 89,10 11 12 14 15,16 18 19 21 23,24 25 27 0

IQR to identify outliers

Qi=9.5 Q3= 23.5
IQR = 23.5 - 9.5 - 4 range of (-11.5 to 44.5)
multilpy IQR by 1.5=14x1.5=21

Qi-21=-11.5
03 + 21 - 5


--- Lect  4 Big Data2 _011245.pdf page 72 ---
Quartiles, The Interquartile Range, And Outliers

a lowest median highest
Finding data values that correspond to Qi, 02, 3 datavalue ‏يم‎ 0 Q; data value
Interquartile Range (IQR) 25% 25% 25% 25%

lIQR = Q3-Qy
IQR = 22-9.5=12.5

Qi=9.5

31 27 25 24 23 21 19 16]17[18 15 14 12 567891011 02155
311170 27 25 23,24 21 19 18 15,16 14 12 11 9,10 8 7 6 5 22= و60

IQR to identify outliers

Qi=9.5 Q3= 23.5
IQR = 23.5 - 9.5 = 4 range of (-11.5 to 44.5)
multilpy IQR by 1.5=14x1.5=21 - any number outside of the range is an outlier
Qi-21=-11.5 outlier =

03 + 21 44.5 any data value smaller than Qi - 1.5(IQR)



--- Lect  4 Big Data2 _011245.pdf page 73 ---
Quartiles, The Interquartile Range, And Outliers

og lowest median highest
Finding data values that correspond to Qi, Q2, 3 datavalue ‏يم‎ Q> ‏ون‎ data value
Interquartile Range (IQR) 25% 25% 25% 25%

IQR = 03- 0

lIQR = 22 - 9,5 - 5

Qi=9.5
Q> - 5 5 6 / 8 9 10 11 12 14 15 16 [17] 18 19 21 23 24 25 27 31
Q3 =22 5 67 89,10 11 12 14 15,16 18 19 21 23,24 25 27 311170

IQR to identify outliers

Qi=9.5 Q3= 23.5
IQR = 23.5 - 9.5 - 4 range of (-11.5 to 44.5)
multilpy IQR by 1.5=14x1.5=21 - any number outside of the range is an outlier
Qi-21=-11.5 outlier =
03 + 21 44.5 any data value smaller than Qi - 1.5(IQR)

any data value larger than 03 + 1.5(IQR)


--- Lect  4 Big Data2 _011245.pdf page 83 ---
Min-Max and Z-Score Normalization - Numerical Example

٠ Use the two methods below to normalize the following

group of data: 1000, 2000, 3000, 5000, 9000
٠ Min-Max Normalization by setting Min=0 and Max=1

٠ Z-Score Normalization


--- Lect  4 Big Data2 _011245.pdf page 84 ---
Min-Max and Z-Score Normalization - Numerical Example

Min-Max Normalization

min = 1000 and Max= 9000



--- Lect  4 Big Data2 _011245.pdf page 85 ---
Min-Max and Z-Score Normalization - Numerical Example

Min-Max Normalization

x-—min

V= eT min = 1000 and Max= 9000 1000
y = 1000-1000 _ P|
~ 9000-1000 ‏عستت‎
‎2000-0 mn |
3000-0
y= 9000-1000 a oe
aan ——————t
5000-0 ‏ظ‎
‎y= =p 9000

eo 1
000 -0
= =1

~ 9000-1000



--- Lect  4 Big Data2 _011245.pdf page 86 ---
Min-Max and Z-Score Normalization - Numerical Example

Normalized Data(v)

Min-Max Normalization

min = 1000 and Max= 9000

= 0.125

O:Z5

0.5

x-—min
max-min

1000-1000 _
9000-1000

2000-0
9000-0

3000 -0
9000-0

5000-1000 _

9000-1000
9000-1000 _

9000-1000

V=

V



--- Lect  4 Big Data2 _011245.pdf page 87 ---
Min-Max and Z-Score Normalization - Numerical Example

Z-Score Normalization

[lL = Mean
OQ = Standard Deviation



--- Lect  4 Big Data2 _011245.pdf page 88 ---
Min-Max and Z-Score Normalization - Numerical Example

Z-Score Normalization

L— pl

all

(1000 + 2000 + 3000 + 5000 + 9000 )
O Mean = = 4000
[lL = Mean

OQ = Standard Deviation



--- Lect  4 Big Data2 _011245.pdf page 89 ---
Min-Max and Z-Score Normalization - Numerical Example

Z-Score Normalization

v— pl
c= ‏مح‎ (1000 + 2000 + 3000 + 5000 + 9000 )
O Mean = oT 2——ia—T = 4000

[lL = Mean
OQ = Standard Deviation

Standard Deviation = ‏عد‎
‎_ (1000 — 4000)? + (2000 — 4000)2 + (3000 — 4000)2 + (5000 — 4000)2 + (9000 — 4000)2
7 | 5-1

= 2489.97



--- Lect  4 Big Data2 _011245.pdf page 90 ---
Min-Max and 7-501 Normalization - Numerical Example

Z-Score Normalization o—
7 (x= ‏م‎ 2 0
oO
[Il — Mean ‏ظ‎
‎Y= aN a = —1,204 O = Standard Deviation |
2489.97 2000
Mean = 4000 3000
Standard Deviation = 2489.97 “4
5000



--- Lect  4 Big Data2 _011245.pdf page 91 ---
Min-Max and Z-Score Normalization - Numerical Example

Normalized Data(v)

Data(v)

L— ‏/م‎

or
land

O

[L — Mean
= —1,204 O = Standard Deviation

= —(0).803 Mean = 4000

Standard Deviation = 2489.97
= —0.4016
= 0.4016

= 2.008

Z-Score Normalization

(x -— 2)
oO

_ 1000-0

2489.97

_ 2000-0

2489.97

_ 3000-0

2489.97

_ 5000-0

2489.97

_ 9000 -0

2489.97



--- Lect  4 Big Data2 _011245.pdf page 110 ---
Simple Methods: Binning
‏ا‎ 11)

Example: customer ages ty number

of values

Equi-width
binning:

Equi-width
binning:

32-38 44-48 55-62


--- Lect  4 Big Data2 _011245.pdf page 111 ---
Data Quality: Handle Noise(Binning)
A

Example: Sorted price values 4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29,

* Partition into three (equi-depth) bins
- Bin 1: 4, 8,9, 15
- Bin 2: 21, 21, 24, 25
- Bin 3: 26, 28, 29, 34
* Smoothing by bin means
- Bin 1:9,9,9,9
- Bin 2: 23, 23, 23, 23
- Bin 3: 29, 29, 29, 29
* Smoothing by bin boundaries
- Bin 1: 4, 4, 4, 15
- Bin 2: 21, 21, 25, 25
- Bin 3: 26, 26, 26, 34


--- Lect  4 Big Data2 _011245.pdf page 114 ---
Feature Scaling ‏تحجيم الميزة‎

import pandas as pd Standard Scaled Data:
from sklearn.preprocessing import StandardScaler, MinMaxScaler Age Income
-1 . 414214 -1.4142714

-8 . 7/6/1867 -8 . /8 7167
# Create a DataFrame

8
1
df = pd.DataFrame(data) 2 2.080006 8. @00000
3
rail

# Sample dataset with features
data = {'Age': [25, 30, 35, 40, 45],
‘Income’: [50000, 60000, 70000, 80000, 90000]}

# Initialize scalers

@./8/16/7 6./8/16/
1.414214 1.414714

standard scaler = StandardScaler()
minmax_scaler = MinMaxScaler()

# Perform feature scaling
scaled data standard = standard_scaler.fit_transform(df) Minkdax Scaled Data:

scaled data_minmax = minmax_scaler.fit_transform(df) Age Income
© . 00 © . 08
© . ‏5ل‎ 8.25
© . 0 © . 58
© . /5 © . /5

1.06 1.60

# Convert scaled data back to DataFrame for visualization

df_ scaled standard = pd.DataFrame(scaled data standard, columns=df.columns)
df scaled minmax = pd.DataFrame(scaled data_minmax, columns=df.columns)

# Display scaled data

print("Standard Scaled Data:")

print(df_scaled_standard)

print("\nMinMax Scaled Data:")

print(df_scaled_minmax)

© غم ‎Woh‏ كل


--- Big Data with Hadoop.pdf page 28 ---
‎Presentation‏ عد

‎Annie’s Answer

‎Ans. Large Data Sets.

‎It is also capable of processing small data-sets. However, to experience
the true power of Hadoop, one needs to have data in TB’s. Because
this is where RDBMS takes hours and fails whereas Hadoop does the
same in couple of minutes.

‎Copyright © edureka and/or its affiliates. All rights reserved.


--- Big Data with Hadoop.pdf page 29 ---
Hadoop Ecosystem



--- Big Data with Hadoop.pdf page 30 ---
‎Presentation‏ عد

‎Hadoop Ecosystem(Hadoop 1.0)

‎Hadoop 1.0

‎Apache Oozie
(Workflow)

‎MapReduce Framework

‎HBase
‎HDFS

‎(Hadoop Distributed File System)
Flume Sqoop
oe 2

‏إلى ا
‎Unstructured or ES‏
‎Semi-structured Data Structured Data‏

‎Copyright © edureka and/or its affiliates. All rights reserved.



--- Big Data with Hadoop.pdf page 31 ---
Hadoop 1.x Limitations

Hadoop 1. x supports only MapReduce-based Batch/Data Processing Applications.

It does not support Real-time data processing.

It allows only one name node and one namespace per cluster to configure, i.e. it does not
support federated architecture.

The entire Hadoop cluster will go down if the name node fails.

The job tracker is the single point of failure that has to perform multiple activities such as
resource management, job scheduling, job monitoring, etc.

Hadoop 1. x does not support horizontal Scaling.

It can support a maximum of 4000 nodes and a maximum of 40,000 concurrent tasks in

the cluster.


--- Big Data with Hadoop.pdf page 32 ---
Apache Hadoop ‏هو نظام جدولة سير العمل لإدارة وظائف‎ Oozie

‎Apache Hive #‏ هو نظام مستودع بيانات موزع ومتسامح مع الأخطاء ويتيح إجراء التحليلات على نطاق
واسع .يوفر ‎Hive Metastore (HMS)‏ مستودعًا مركزيًا للبيانات التعريفية التي يمكن تحليلها بسهولة لاتخاذ
قرارات مستنيرة ومعتمدة على البيانات» وبالتالي فهو مكون حاسم في العديد من بنيات مستودعات البيانات.

‎Apache Pig #‏ عبارة عن منصة لتحليل مجموعات البيانات الكبيرة التي تتكون من لغة عالية المستوى للتعبير
عن برامج تحليل البيانات» إلى ‎Gils‏ البنية التحتية لتقييم هذه البرامج. الخاصية البارزة لبرامج ‎Pig‏ أن
بنيتها قابلة للتوازي الكبيرء ‎Gilly‏ بدوره ‎Sas‏ من التعامل مع مجموعات كبيرة ‎Ma‏ من البيانات.


--- Big Data with Hadoop.pdf page 33 ---
‎Apache 73011] "“‏ هو إطار جبر خطي موزع و ‎DSL‏ 18ح 95معبر رياضيًا مصمم للسماح لعلماء
الرياضيات والإحصائيين وعلماء البيانات بتنفيذ خوارزمياتهم الخاصة بسرعة.

‏# يقوم ‎Flume‏ بنقل ملفات السجل الأولية عن طريق سحبها من مصادر متعددة وتدفقها إلى نظام ملفات
‎«ellis Hadoop.‏ يمكن استهلاك ملفات السجل بواسطة أدوات تحليلية مثل ‎Spark‏ أو ‎Kafka.‏ يمكن ل
716 1الاتصال بمكونات إضافية مختلفة لضمان دفع بيانات السجل إلى ‎Age oll‏ الصحيحة.

‏د مو0نن5 هي أداة تستخدم لنقل البيانات المجمعة بين 113000]ومخازن البيانات الخارجية» مثل قواعد البيانات
العلائقية ‎«MS SQL Server)‏ .(.17:501لمعالجة البيانات باستخدام 113000 يجب أولاً تحميل البيانات

‏إلى مجموعات م1132000]من عدة مصادر.


--- Big Data with Hadoop.pdf page 34 ---
2 Presentation

‎»sHBase‏ نظام
إدارة قواعد بيانات غير

‏الأعمدة يعمل أعلى
نظام الملفات الموزعة
‎Hadoop (HDFS).‏
يوفر ‎4a; bHBase‏
متسامحة مع الأخطاء

‎Hadoop Ecosystem(Hadoop 2.0)

‏الفكرة الأساسية ل
‎YARN‏ هي تقسيم إدارة

‏لديك مدير موارد عام

‎(RM)‏ ومدير تطبيق

‏البيانات المتفرقة» وهي رئيسي ‎(AM)‏ لكل
إما وظيفة واحدة أو

‏حالات استخدام

‎Copyright © edureka and/or its affiliates. All rights reserved.

‎Unstructured or

‎Semi-structured Data

‎ii

‎a
‎a
‎J

‎Structured Data



--- Big Data with Hadoop.pdf page 35 ---
2 Presentation

Machine Learning with Mahout

Write Intelligent Applications Using Apache Mahout
Linkedin Recommendations

COMPANIES YOU MAY WANT TO FOLLOW JOBS YOU MAY BE INTERESTED IN

Manager Global Infra Pre

, 1 0 1 "Cf Sales .
Hadoop and MapReduce A YAHOO!‘ rupa HCL Technologies - Noida

Magic in Action

Adobe
5 Sr Account Manager,
Pivotal pivotal :
Pivotal Inc. - India - Mumbai
‏]لع امن‎ —_—- Foursquare

Solution Sales Specialist
TERADATA Teradata - IN-Karnataka-Ban_.

Feedback | See more »

Source: https://mahout.apache.org/general/powered-by-mahout.html



--- Big Data with Hadoop.pdf page 36 ---
Hadoop 2.x Core Components



--- Big Data with Hadoop.pdf page 37 ---
‎Presentation‏ عد

‎Hadoop 2.x Core Components

‎Hadoop 2.x Core Components

‎Storage Processing

‎T
‎¥

‎vase ‏له‎
‎Slave Node Manager

‎Secondary
‎NameNode

‎Copyright © edureka and/or its affiliates. All rights reserved.



--- Big Data with Hadoop.pdf page 38 ---
‎Presentation‏ عد

‎Hadoop 2.x Core Components (Contd.)

‎Resource
‎Manager

‎HDFS
‎Cluster NameNode

‎Copyright © edureka and/or its affiliates. All rights reserved.



--- Big Data with Hadoop.pdf page 39 ---
2 Presentation

Core Components of HDFS

= MaterNode Master Node

= Maintains and manages the blocks which are present on the

DataNodes

= Slave nodes which are deployed on each machine and is

responsible for storing the data

" Responsible for serving read and write requests for the clients



--- Big Data with Hadoop.pdf page 40 ---
NameNode
(Stores metadata only)

METADATA:

/user/doug/hinfo ->135
/user/doug/pdetail -> 4 2

NameNode:

Keeps track of overall file directory structure
and the placement of Data Block

Copyright © edureka and/or its affiliates. All rights reserved.

‎Presentation‏ عد

‎NameNode Metadata

‎Meta-data in Memory

‎=" The entire metadata is in the main memory

‎=" No demand paging of FS meta-data

‎Types of Metadata
" List of files
* List of blocks for each file

‎=" List of DataNode for each block

‎" File attributes, e.g. access time,replication factor, etc.

‎A Transaction Log

‎=" Records file creations, file deletions etc.

‎edureka!


